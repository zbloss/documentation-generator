{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import base64\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OAuth2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../git_secrets.json', 'r') as file:\n",
    "    secrets = json.loads(file.read())\n",
    "    file.close()\n",
    "\n",
    "client_id = secrets['client_id']\n",
    "client_secret = secrets['client_secret']\n",
    "pat = secrets['pat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"message\":\"Not Found\",\"documentation_url\":\"https://developer.github.com/v3\"}\n"
     ]
    }
   ],
   "source": [
    "auth_params = {\n",
    "    'username': client_id,\n",
    "    'token': pat,\n",
    "    'client_id': client_id,\n",
    "    'client_secret': client_secret,\n",
    "    'redirect_uri': 'http://localhost:1234/path'\n",
    "}\n",
    "\n",
    "response = requests.post('https://api.github.com/login/oauth/authorize', headers=auth_params)\n",
    "print(response.content.decode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "curl_string = f'{client_id}:{client_secret}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'120fd2f1da4a2f1d5351:5e876ff7d1f1561b94b0d57adc005c4667a10556'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "curl_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get('https://api.github.com/')\n",
    "content = response.content.decode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'message': \"API rate limit exceeded for 98.224.131.229. (But here's the good news: Authenticated requests get a higher rate limit. Check out the documentation for more details.)\",\n",
       " 'documentation_url': 'https://developer.github.com/v3/#rate-limiting'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json.loads(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New Goal\n",
    "\n",
    "The new goal is to build a function that given an owner and a repository, returns all of the python files within 2 subdirectories of that repository\n",
    "\n",
    "`github v3 api: /repos/:owner/:repo/contents`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "response = requests.get('https://api.github.com/repos/zbloss/TransformerModel/contents')\n",
    "content = json.loads(response.content.decode())\n",
    "print(type(content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_python_files(owner: str, repo: str):\n",
    "    \"\"\"\n",
    "    Given a Github owner and repository, constructs a valid Github V3 api call and \n",
    "    returns a list of file objects in the provided directory.\n",
    "    \n",
    "    :param owner: (str) The username associated with the repository.\n",
    "    :param repo: (str) The repository to pull from Github.\n",
    "    :returns: list of dictionaries containing urls to the python files\n",
    "    \"\"\"\n",
    "    dirs = []\n",
    "    files = []\n",
    "    python_files_to_ignore = ['__init__.py', 'setup.py']\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(f'https://api.github.com/repos/{owner}/{repo}/contents')\n",
    "        assert response.status_code == 200\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f'Requests Exception: {e}')\n",
    "    \n",
    "    content = response.content\n",
    "    try:\n",
    "        content = json.loads(content)\n",
    "    except Exception as e:\n",
    "        print(f'JSON Loads Exception: {e}')\n",
    "        \n",
    "    for item in content:\n",
    "        # adding python files to the list of files\n",
    "        if item['type'] == 'file' and \\\n",
    "           item['path'][-3:] == '.py' and \\\n",
    "           item['path'] not in python_files_to_ignore:\n",
    "            item.pop('name')\n",
    "            files.append(item)\n",
    "            \n",
    "        # searching the directory trees for files \n",
    "        # and appending them to files.\n",
    "        elif item['type'] == 'dir':\n",
    "            \n",
    "            if 'git_url' in list(item.keys()):\n",
    "                tree = item['git_url']\n",
    "                response  = requests.get(tree)\n",
    "                content = json.loads(response.content.decode())\n",
    "\n",
    "                if type(content['tree']) == type([]):\n",
    "\n",
    "                    for file in content['tree']:\n",
    "                        # This will check the last 3 characters of the filename to see if they are '.py'\n",
    "                        # indicating a python file.\n",
    "                        if file['path'][-3:] == '.py' and file['path'] not in python_files_to_ignore:\n",
    "                            files.append(file)\n",
    "                                                 \n",
    "    return files\n",
    "\n",
    "def get_python_data(py_files: list):\n",
    "    \"\"\"\n",
    "    Given a list of valid Github V3 api called python files, returns a dictionary\n",
    "    containing the filename as the key and code as the value.\n",
    "    \n",
    "    :param py_files: (list) list of dictionaries containing a `url` key.\n",
    "    :returns: dictionary containing filename and code.\n",
    "    \"\"\"\n",
    "    py_data = {}\n",
    "    \n",
    "    for item in py_files:\n",
    "        if 'url' in list(item.keys()):\n",
    "            \n",
    "            try:\n",
    "                response = requests.get(item['url'])\n",
    "                content = response.content.decode()\n",
    "            except Exception as e:\n",
    "                print(f'Requests Exception: {e}')\n",
    "                pass\n",
    "            \n",
    "            # Grabbing the Base64 encoded python file\n",
    "            try:\n",
    "                content = json.loads(content)\n",
    "                content = content['content']\n",
    "            except Exception as e:\n",
    "                print(f'JSON Loads Exception: {e}')\n",
    "                pass\n",
    "            \n",
    "            py = base64.b64decode(content)\n",
    "            \n",
    "            py_data[item['path']] = py.decode()\n",
    "            \n",
    "    return py_data\n",
    "\n",
    "def python_code_cleaner(code: str):\n",
    "    \"\"\"\n",
    "    Given the python code as a string, pads around the '\\n' characters,\n",
    "    replaces '    ' with <TAB>, and replaces \\\"\\\"\\\" with [DOCSTRING].\n",
    "    \n",
    "    :param code: (str) python code as a string.\n",
    "    :returns: python code as a string\n",
    "    \"\"\"\n",
    "    \n",
    "    pad_n = re.sub('\\n', ' \\n ', code)\n",
    "    pad_tab = re.sub(' '*4, '[TAB]', pad_n)\n",
    "    pad_docstring = re.sub('\\\"\\\"\\\"', '[DOCSTRING]', pad_tab)\n",
    "    \n",
    "    return pad_docstring\n",
    "\n",
    "def docstring_extractor(code: str):\n",
    "    \"\"\"\n",
    "    Given cleaned python function as a string, extracts the docstring and returns a tuple\n",
    "    containing the docstring and the function with the docstring removed.\n",
    "    \n",
    "    :param code: (str) python function as a string.\n",
    "    :returns: (tuple) of (docstring, python function)\n",
    "    \"\"\"\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requests Exception: \n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "string indices must be integers",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-135-6c4c1e3803ab>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtransformer_files\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_python_files\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mowner\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'zbloss'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrepo\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'TransformerModel'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'Retreived {len(transformer_files)} Python Files'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-134-7702fe094735>\u001b[0m in \u001b[0;36mget_python_files\u001b[1;34m(owner, repo)\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcontent\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[1;31m# adding python files to the list of files\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0mitem\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'type'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'file'\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m            \u001b[0mitem\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'path'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'.py'\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m            \u001b[0mitem\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'path'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpython_files_to_ignore\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: string indices must be integers"
     ]
    }
   ],
   "source": [
    "transformer_files = get_python_files(owner='zbloss', repo='TransformerModel')\n",
    "print(f'Retreived {len(transformer_files)} Python Files')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['example.py', 'attention.py', 'custom_scheduler.py', 'data_process.py', 'decoder.py', 'decoder_layer.py', 'encoder.py', 'encoder_layer.py', 'evaluate.py', 'masker.py', 'positional_encoder.py', 'training.py', 'transformer.py', 'transformer_xl.py'])"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "python_files.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON Loads Exception: 'content'\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "argument should be a bytes-like object or ASCII string, not 'dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-130-1faff26fd30f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpython_files\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_python_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtransformer_files\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'Retrieved code from {len(python_files)} files'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-128-650e2594f96a>\u001b[0m in \u001b[0;36mget_python_data\u001b[1;34m(py_files)\u001b[0m\n\u001b[0;32m     80\u001b[0m                 \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 82\u001b[1;33m             \u001b[0mpy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbase64\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mb64decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     83\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m             \u001b[0mpy_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'path'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\altoz\\appdata\\local\\programs\\python\\python37\\lib\\base64.py\u001b[0m in \u001b[0;36mb64decode\u001b[1;34m(s, altchars, validate)\u001b[0m\n\u001b[0;32m     78\u001b[0m     \u001b[1;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0minput\u001b[0m \u001b[0mresult\u001b[0m \u001b[1;32min\u001b[0m \u001b[0ma\u001b[0m \u001b[0mbinascii\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mError\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m     \"\"\"\n\u001b[1;32m---> 80\u001b[1;33m     \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_bytes_from_decode_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     81\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0maltchars\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m         \u001b[0maltchars\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_bytes_from_decode_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maltchars\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\altoz\\appdata\\local\\programs\\python\\python37\\lib\\base64.py\u001b[0m in \u001b[0;36m_bytes_from_decode_data\u001b[1;34m(s)\u001b[0m\n\u001b[0;32m     44\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m         raise TypeError(\"argument should be a bytes-like object or ASCII \"\n\u001b[1;32m---> 46\u001b[1;33m                         \"string, not %r\" % s.__class__.__name__) from None\n\u001b[0m\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: argument should be a bytes-like object or ASCII string, not 'dict'"
     ]
    }
   ],
   "source": [
    "python_files = get_python_data(transformer_files)\n",
    "print(f'Retrieved code from {len(python_files)} files')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python_files.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import tensorflow as tf \n",
      "  \n",
      "  \n",
      " class MultiHeadAttention(tf.keras.layers.Layer): \n",
      "  \n",
      "[TAB] def __init__(self, d_model, num_heads): \n",
      "[TAB][TAB] \"\"\" \n",
      "[TAB][TAB] :param d_model: The Dimensionality of the Attention mechanism \n",
      "[TAB][TAB] :param num_heads: The number of heads to use in the attention mechanism \n",
      "[TAB][TAB] \"\"\" \n",
      "[TAB][TAB] super(MultiHeadAttention, self).__init__() \n",
      "[TAB][TAB] self.num_heads = num_heads \n",
      "[TAB][TAB] self.d_model = d_model \n",
      "  \n",
      "[TAB][TAB] assert d_model % self.num_heads == 0 \n",
      "  \n",
      "[TAB][TAB] self.depth = d_model // self.num_heads \n",
      "  \n",
      "[TAB][TAB] self.wq = tf.keras.layers.Dense(d_model) \n",
      "[TAB][TAB] self.wk = tf.keras.layers.Dense(d_model) \n",
      "[TAB][TAB] self.wv = tf.keras.layers.Dense(d_model) \n",
      "  \n",
      "[TAB][TAB] self.dense = tf.keras.layers.Dense(d_model) \n",
      "  \n",
      "[TAB] @staticmethod \n",
      "[TAB] def scaled_dot_product_attention(q, k, v, mask): \n",
      "[TAB][TAB] \"\"\" \n",
      "[TAB][TAB] :param q: The query vector \n",
      "[TAB][TAB] :param k: The key vector \n",
      "[TAB][TAB] :param v: The value vector \n",
      "[TAB][TAB] :param mask: Float tensor with shape (..., seq_len_q, seq_len_k). Defaults to None. \n",
      "[TAB][TAB] :return: the scaled dot product output and attention weights (output, attention_weights) \n",
      "[TAB][TAB] \"\"\" \n",
      "  \n",
      "[TAB][TAB] matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k) \n",
      "  \n",
      "[TAB][TAB] # scale matmul_qk \n",
      "[TAB][TAB] dk = tf.cast(tf.shape(k)[-1], tf.float32) \n",
      "[TAB][TAB] scaled_attention_logits = matmul_qk / tf.math.sqrt(dk) \n",
      "  \n",
      "[TAB][TAB] # add the mask to the scaled tensor. \n",
      "[TAB][TAB] if mask is not None: \n",
      "[TAB][TAB][TAB] scaled_attention_logits += (mask * -1e9) \n",
      "  \n",
      "[TAB][TAB] # softmax is normalized on the last axis (seq_len_k) so that the scores \n",
      "[TAB][TAB] # add up to 1. \n",
      "[TAB][TAB] attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k) \n",
      "  \n",
      "[TAB][TAB] output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v) \n",
      "  \n",
      "[TAB][TAB] return output, attention_weights \n",
      "  \n",
      "[TAB] def split_heads(self, x, batch_size): \n",
      "[TAB][TAB] \"\"\" \n",
      "[TAB][TAB] :param x: the head input \n",
      "[TAB][TAB] :param batch_size: \n",
      "[TAB][TAB] :return: splits the last dimension and returns the result with shape (batch_size, num_heads, seq_length, depth) \n",
      "[TAB][TAB] \"\"\" \n",
      "[TAB][TAB] x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth)) \n",
      "[TAB][TAB] return tf.transpose(x, perm=[0, 2, 1, 3]) \n",
      "  \n",
      "[TAB] def call(self, v, k, q, mask): \n",
      "[TAB][TAB] \"\"\" \n",
      "[TAB][TAB] :param v: Value vector \n",
      "[TAB][TAB] :param k: Key vector \n",
      "[TAB][TAB] :param q: Query vector \n",
      "[TAB][TAB] :param mask: Mask shape to use \n",
      "[TAB][TAB] :return: \n",
      "[TAB][TAB] \"\"\" \n",
      "[TAB][TAB] batch_size = tf.shape(q)[0] \n",
      "  \n",
      "[TAB][TAB] q = self.wq(q)  # (batch_size, seq_len, d_model) \n",
      "[TAB][TAB] k = self.wk(k)  # (batch_size, seq_len, d_model) \n",
      "[TAB][TAB] v = self.wv(v)  # (batch_size, seq_len, d_model) \n",
      "  \n",
      "[TAB][TAB] q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth) \n",
      "[TAB][TAB] k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth) \n",
      "[TAB][TAB] v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth) \n",
      "  \n",
      "[TAB][TAB] # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth) \n",
      "[TAB][TAB] # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k) \n",
      "[TAB][TAB] scaled_attention, attention_weights = self.scaled_dot_product_attention( \n",
      "[TAB][TAB][TAB] q, k, v, mask) \n",
      "  \n",
      "[TAB][TAB] scaled_attention = tf.transpose(scaled_attention, \n",
      "[TAB][TAB][TAB][TAB][TAB][TAB][TAB][TAB][TAB][TAB] perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth) \n",
      "  \n",
      "[TAB][TAB] concat_attention = tf.reshape(scaled_attention, \n",
      "[TAB][TAB][TAB][TAB][TAB][TAB][TAB][TAB][TAB]   (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model) \n",
      "  \n",
      "[TAB][TAB] output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model) \n",
      "  \n",
      "[TAB][TAB] return output, attention_weights \n",
      "  \n",
      "[TAB] @staticmethod \n",
      "[TAB] def point_wise_feed_forward_network(d_model, dff): \n",
      "[TAB][TAB] \"\"\" \n",
      "[TAB][TAB] :param d_model: \n",
      "[TAB][TAB] :param dff: \n",
      "[TAB][TAB] :return: A feed forward nn to be stacked after each attention layer. \n",
      "[TAB][TAB] \"\"\" \n",
      "  \n",
      "[TAB][TAB] return tf.keras.Sequential([ \n",
      "[TAB][TAB][TAB] tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff) \n",
      "[TAB][TAB][TAB] tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model) \n",
      "[TAB][TAB] ]) \n",
      " \n"
     ]
    }
   ],
   "source": [
    "clean_code = python_code_cleaner(python_files['attention.py'])\n",
    "print(clean_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import tensorflow as tf \n",
      "  \n",
      "  \n",
      " class MultiHeadAttention(tf.keras.layers.Layer): \n",
      "  \n",
      "[TAB] def __init__(self, d_model, num_heads): \n",
      "[TAB][TAB] [DOCSTRING] \n",
      "[TAB][TAB] :param d_model: The Dimensionality of the Attention mechanism \n",
      "[TAB][TAB] :param num_heads: The number of heads to use in the attention mechanism \n",
      "[TAB][TAB] [DOCSTRING] \n",
      "[TAB][TAB] super(MultiHeadAttention, self).__init__() \n",
      "[TAB][TAB] self.num_heads = num_heads \n",
      "[TAB][TAB] self.d_model = d_model \n",
      "  \n",
      "[TAB][TAB] assert d_model % self.num_heads == 0 \n",
      "  \n",
      "[TAB][TAB] self.depth = d_model // self.num_heads \n",
      "  \n",
      "[TAB][TAB] self.wq = tf.keras.layers.Dense(d_model) \n",
      "[TAB][TAB] self.wk = tf.keras.layers.Dense(d_model) \n",
      "[TAB][TAB] self.wv = tf.keras.layers.Dense(d_model) \n",
      "  \n",
      "[TAB][TAB] self.dense = tf.keras.layers.Dense(d_model) \n",
      "  \n",
      "[TAB] @staticmethod \n",
      "[TAB] def scaled_dot_product_attention(q, k, v, mask): \n",
      "[TAB][TAB] [DOCSTRING] \n",
      "[TAB][TAB] :param q: The query vector \n",
      "[TAB][TAB] :param k: The key vector \n",
      "[TAB][TAB] :param v: The value vector \n",
      "[TAB][TAB] :param mask: Float tensor with shape (..., seq_len_q, seq_len_k). Defaults to None. \n",
      "[TAB][TAB] :return: the scaled dot product output and attention weights (output, attention_weights) \n",
      "[TAB][TAB] [DOCSTRING] \n",
      "  \n",
      "[TAB][TAB] matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k) \n",
      "  \n",
      "[TAB][TAB] # scale matmul_qk \n",
      "[TAB][TAB] dk = tf.cast(tf.shape(k)[-1], tf.float32) \n",
      "[TAB][TAB] scaled_attention_logits = matmul_qk / tf.math.sqrt(dk) \n",
      "  \n",
      "[TAB][TAB] # add the mask to the scaled tensor. \n",
      "[TAB][TAB] if mask is not None: \n",
      "[TAB][TAB][TAB] scaled_attention_logits += (mask * -1e9) \n",
      "  \n",
      "[TAB][TAB] # softmax is normalized on the last axis (seq_len_k) so that the scores \n",
      "[TAB][TAB] # add up to 1. \n",
      "[TAB][TAB] attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k) \n",
      "  \n",
      "[TAB][TAB] output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v) \n",
      "  \n",
      "[TAB][TAB] return output, attention_weights \n",
      "  \n",
      "[TAB] def split_heads(self, x, batch_size): \n",
      "[TAB][TAB] [DOCSTRING] \n",
      "[TAB][TAB] :param x: the head input \n",
      "[TAB][TAB] :param batch_size: \n",
      "[TAB][TAB] :return: splits the last dimension and returns the result with shape (batch_size, num_heads, seq_length, depth) \n",
      "[TAB][TAB] [DOCSTRING] \n",
      "[TAB][TAB] x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth)) \n",
      "[TAB][TAB] return tf.transpose(x, perm=[0, 2, 1, 3]) \n",
      "  \n",
      "[TAB] def call(self, v, k, q, mask): \n",
      "[TAB][TAB] [DOCSTRING] \n",
      "[TAB][TAB] :param v: Value vector \n",
      "[TAB][TAB] :param k: Key vector \n",
      "[TAB][TAB] :param q: Query vector \n",
      "[TAB][TAB] :param mask: Mask shape to use \n",
      "[TAB][TAB] :return: \n",
      "[TAB][TAB] [DOCSTRING] \n",
      "[TAB][TAB] batch_size = tf.shape(q)[0] \n",
      "  \n",
      "[TAB][TAB] q = self.wq(q)  # (batch_size, seq_len, d_model) \n",
      "[TAB][TAB] k = self.wk(k)  # (batch_size, seq_len, d_model) \n",
      "[TAB][TAB] v = self.wv(v)  # (batch_size, seq_len, d_model) \n",
      "  \n",
      "[TAB][TAB] q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth) \n",
      "[TAB][TAB] k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth) \n",
      "[TAB][TAB] v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth) \n",
      "  \n",
      "[TAB][TAB] # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth) \n",
      "[TAB][TAB] # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k) \n",
      "[TAB][TAB] scaled_attention, attention_weights = self.scaled_dot_product_attention( \n",
      "[TAB][TAB][TAB] q, k, v, mask) \n",
      "  \n",
      "[TAB][TAB] scaled_attention = tf.transpose(scaled_attention, \n",
      "[TAB][TAB][TAB][TAB][TAB][TAB][TAB][TAB][TAB][TAB] perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth) \n",
      "  \n",
      "[TAB][TAB] concat_attention = tf.reshape(scaled_attention, \n",
      "[TAB][TAB][TAB][TAB][TAB][TAB][TAB][TAB][TAB]   (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model) \n",
      "  \n",
      "[TAB][TAB] output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model) \n",
      "  \n",
      "[TAB][TAB] return output, attention_weights \n",
      "  \n",
      "[TAB] @staticmethod \n",
      "[TAB] def point_wise_feed_forward_network(d_model, dff): \n",
      "[TAB][TAB] [DOCSTRING] \n",
      "[TAB][TAB] :param d_model: \n",
      "[TAB][TAB] :param dff: \n",
      "[TAB][TAB] :return: A feed forward nn to be stacked after each attention layer. \n",
      "[TAB][TAB] [DOCSTRING] \n",
      "  \n",
      "[TAB][TAB] return tf.keras.Sequential([ \n",
      "[TAB][TAB][TAB] tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff) \n",
      "[TAB][TAB][TAB] tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model) \n",
      "[TAB][TAB] ]) \n",
      " \n"
     ]
    }
   ],
   "source": [
    "print(re.sub('\\\"\\\"\\\"', '[DOCSTRING]', clean_code))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
